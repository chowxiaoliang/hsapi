val lines = sc.textFile("README.md")
1.filter()
    val filterLines = lines.filter(line => line.contains("filter"))
2.distinct()
    val disLines = lines.distinct()
    val rdd = sc.makeRDD(Array("hello","hello","hello","world"))
    rdd.distinct().foreach {println}
    /*
    hello
    world
    */
3.map() map以一条记录为单位进行操作
    val mapLines = lines.map(w => (w,1))
4.flatMap()
    val flatMapLines = lines.flatMap(x => x.split(","))
    map 和 flatMap的区别
    map：输入一条数据，返回一条数据
    flatMap：输入一条数据，可能返回多条数据
5.mapPartitions() 以分区为单位进行操作
    val arr = Array("Tom","Bob","Tony","Jerry")
    //把4条数据分到两个分区中
    val rdd = sc.parallelize(arr,2)
    /*
    * 将RDD中的数据写入到数据库中，绝大部分使用mapPartitions算子来实现
    */
    val mapPartitionsRdd = rdd.mapPartitions(x => {
      println("创建数据库连接...")
      val list = new ListBuffer[String]()
      while(x.hasNext) {
        // 模拟写入数据库
        list += x.next() + "写入数据库"
      }
      // 模拟执行SQL语句，批量插入
      list.iterator
    })
6.mapPartitionsWithIndex
    val dataArr = Array("Tom01","Tom02","Tom03","Tom04","Tom05","Tom06","Tom07","Tom08","Tom09","Tom10","Tom11","Tom12")
    val rdd = sc.parallelize(dataArr, 3);
    val result = rdd.mapPartitionsWithIndex((index,x) => {
        val list = ListBuffer[String]()
        while (x.hasNext) {
          list += "partition:"+ index + " content:" + x.next
        }
        list.iterator
    })
    println("分区数量:" + result.partitions.size)
    val resultArr = result.collect()
    for(x <- resultArr){
      println(x)
    }
7.sample 随机取样
    sample(withReplacement: Boolean, fraction: Double, seed: Long)

    withReplacement : 是否是放回式抽样
        true代表如果抽中A元素，之后还可以抽取A元素
        false代表如果抽中了A元素，之后都不在抽取A元素
    fraction : 抽样的比例
    seed : 抽样算法的随机数种子，不同的数值代表不同的抽样规则，可以手动设置，默认为long的随机数

    val rdd = sc.makeRDD(Array(
      "hello1","hello2","hello3","hello4","hello5","hello6","world1","world2","world3","world4"))
    rdd.sample(false, 0.3).foreach(println)
    结果：理论上会随机抽取30%的数据，但是在数据量不大的时候，不一定很准确
8.union 把两个RDD进行逻辑上的合并
    val rdd1 =sc.makeRDD(1 to 3)
    val rdd2 = sc.parallelize(4 until 6)
    val unionRdd = rdd1.union(rdd2)
9.intersection 求两个RDD的交集
    val rdd1 =sc.makeRDD(1 to 3)
    val rdd2 = sc.parallelize(2 to 5)
    val interSectionRdd = rdd1.intersection(rdd2)
10.sortBy 手动指定排序的字段（_表示一个整体）
    val arr = Array(Tuple3(190,100,"Jed"),Tuple3(100,202,"Tom"),Tuple3(90,111,"Tony"))
    val rdd = sc.parallelize(arr)
    rdd.sortBy(_._1).foreach(println)
    /* 按第一个元素排序
       (90,111,Tony)
       (100,202,Tom)
       (190,100,Jed)
     */
    rdd.sortBy(_._2, false).foreach(println)
    /* 按照第二个元素排序，降序
       (100,202,Tom)
       (90,111,Tony)
       (190,100,Jed)
     */
    rdd.sortBy(_._3).foreach(println)
    /* 按照第三个元素排序
       (190,100,Jed)
       (100,202,Tom)
       (90,111,Tony)
     */
    }
11.sortByKey 按key进行排序
    val rdd = sc.makeRDD(Array((5,"Tom"),(10,"Jed"),(3,"Tony"),(2,"Jack")))
    rdd.sortByKey().foreach(println)
    结果：
    (2,Jack)
    (3,Tony)
    (5,Tom)
    (10,Jed)
12.groupByKey reduceByKey
    val rdd = sc.makeRDD(Array(("Tom",1),("Tom",1),("Tony",1),("Tony",1)))
    rdd.groupByKey().foreach(println)
    /*
    (Tom,CompactBuffer(1, 1))
    (Tony,CompactBuffer(1, 1))
    */
    rdd.reduceByKey(_+_).foreach(println)
    /*
    (Tom,2)
    (Tony,2)
    */
    区别：Hello,1 World,1 Hadoop,1 Hello,1 Hello,1 Hello,1 World,1 Hadoop,1 World,1
    groupByKey => Hello[1,1,1,1] World[1,1,1] Hadoop[1,1]
    reduceByKey => Hello,4 World,3 Hadoop,2
13.coalesce、repartition 改变RDD分区数
    /*
     * false:不产生shuffle
     * true:产生shuffle
     * 如果重分区的数量大于原来的分区数量,必须设置为true,否则分区数不变
     * 增加分区会把原来的分区中的数据随机分配给设置的分区中
     * 默认为false
     */
    object CoalesceTest {
      def main(args: Array[String]): Unit = {
        val conf = new SparkConf().setAppName("MapTest").setMaster("local")
        val sc = new SparkContext(conf)
        val arr = Array(
          "partition:0 content:Tom01",
          "partition:0 content:Tom02",
          "partition:0 content:Tom03",
          "partition:0 content:Tom04",
          "partition:1 content:Tom05",
          "partition:1 content:Tom06",
          "partition:1 content:Tom07",
          "partition:1 content:Tom08",
          "partition:2 content:Tom09",
          "partition:2 content:Tom10",
          "partition:2 content:Tom11",
          "partition:2 content:Tom12")

        val rdd = sc.parallelize(arr, 3);

        val coalesceRdd = rdd.coalesce(6,true)

        val results = coalesceRdd.mapPartitionsWithIndex((index,x) => {
          val list = ListBuffer[String]()
          while (x.hasNext) {
            list += "partition:"+ index + " content:[" + x.next + "]"
          }
          list.iterator
        })
        println("分区数量:" + results.partitions.size)
        results.foreach(println)
        /*
        分区数量:6
        partition:0 content:[partition:1 content:Tom07]
        partition:0 content:[partition:2 content:Tom10]
        partition:1 content:[partition:0 content:Tom01]
        partition:1 content:[partition:1 content:Tom08]
        partition:1 content:[partition:2 content:Tom11]
        partition:2 content:[partition:0 content:Tom02]
        partition:2 content:[partition:2 content:Tom12]
        partition:3 content:[partition:0 content:Tom03]
        partition:4 content:[partition:0 content:Tom04]
        partition:4 content:[partition:1 content:Tom05]
        partition:5 content:[partition:1 content:Tom06]
        partition:5 content:[partition:2 content:Tom09]
        */
        // 增加分区肯定会发生shuffle，如果设置为false，结果是不变的
        val coalesceRdd2 = rdd.coalesce(6,false)
        val results2 = coalesceRdd2.mapPartitionsWithIndex((index,x) => {
          val list = ListBuffer[String]()
          while (x.hasNext) {
            list += "partition:"+ index + " content:[" + x.next + "]"
          }
          list.iterator
        })
        println("分区数量:" + results2.partitions.size)
        results2.foreach(println)
        /*
        分区数量:3
        partition:0 content:[partition:0 content:Tom01]
        partition:0 content:[partition:0 content:Tom02]
        partition:0 content:[partition:0 content:Tom03]
        partition:0 content:[partition:0 content:Tom04]
        partition:1 content:[partition:1 content:Tom05]
        partition:1 content:[partition:1 content:Tom06]
        partition:1 content:[partition:1 content:Tom07]
        partition:1 content:[partition:1 content:Tom08]
        partition:2 content:[partition:2 content:Tom09]
        partition:2 content:[partition:2 content:Tom10]
        partition:2 content:[partition:2 content:Tom11]
        partition:2 content:[partition:2 content:Tom12]
        */
        val coalesceRdd3 = rdd.coalesce(2,true)
        val results3 = coalesceRdd3.mapPartitionsWithIndex((index,x) => {
          val list = ListBuffer[String]()
          while (x.hasNext) {
            list += "partition:"+ index + " content:[" + x.next + "]"
          }
          list.iterator
        })
        println("分区数量:" + results3.partitions.size)
        results3.foreach(println)
        /*
        分区数量:2
        partition:0 content:[partition:0 content:Tom01]
        partition:0 content:[partition:0 content:Tom03]
        partition:0 content:[partition:1 content:Tom05]
        partition:0 content:[partition:1 content:Tom07]
        partition:0 content:[partition:2 content:Tom09]
        partition:0 content:[partition:2 content:Tom11]
        partition:1 content:[partition:0 content:Tom02]
        partition:1 content:[partition:0 content:Tom04]
        partition:1 content:[partition:1 content:Tom06]
        partition:1 content:[partition:1 content:Tom08]
        partition:1 content:[partition:2 content:Tom10]
        partition:1 content:[partition:2 content:Tom12]
        */
        val coalesceRdd4 = rdd.coalesce(2,false)
        val results4 = coalesceRdd4.mapPartitionsWithIndex((index,x) => {
          val list = ListBuffer[String]()
          while (x.hasNext) {
            list += "partition:"+ index + " content:[" + x.next + "]"
          }
          list.iterator
        })

        println("分区数量:" + results4.partitions.size)
        results4.foreach(println)
        /*
        分区数量:2
        partition:0 content:[partition:0 content:Tom01]
        partition:0 content:[partition:0 content:Tom02]
        partition:0 content:[partition:0 content:Tom03]
        partition:0 content:[partition:0 content:Tom04]
        partition:1 content:[partition:1 content:Tom05]
        partition:1 content:[partition:1 content:Tom06]
        partition:1 content:[partition:1 content:Tom07]
        partition:1 content:[partition:1 content:Tom08]
        partition:1 content:[partition:2 content:Tom09]
        partition:1 content:[partition:2 content:Tom10]
        partition:1 content:[partition:2 content:Tom11]
        partition:1 content:[partition:2 content:Tom12]
        */
      }
    }
    和repartition的区别：repartition(int n) = coalesce(int n, true)
14.partitionBy 自定义分区器，重新分区
    package com.aura.transformations
    import org.apache.spark.{Partitioner, SparkConf, SparkContext}
    /**
      * Author: zhouliang
      * Description: 自定义分区规则
      * Date: Create in 2018/1/12
      */
    class MyPartition extends Partitioner {

      // 分区数量为2
      override def numPartitions: Int = 2

      // 自定义分区规则
      override def getPartition(key: Any): Int = {
        if(key.hashCode() % 2 == 0) {
          0
        }else {
          1
        }
      }
    }

    object PartitionByTest {

      def main(args: Array[String]): Unit = {
        val conf = new SparkConf().setAppName("PartitionByTest").setMaster("local")
        val sc = new SparkContext(conf)

        val arr = Array((1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9))
        val rdd = sc.makeRDD(arr,3)
        println("分区数量：" + rdd.partitions.length)
        rdd.foreachPartition(x => {
          println("*******")
          while(x.hasNext) {
            println(x.next())
          }
        })
        /*
        分区数量：3
        *******
        (1,1)
        (2,2)
        (3,3)
        *******
        (4,4)
        (5,5)
        (6,6)
        *******
        (7,7)
        (8,8)
        (9,9)
         */
        val partitionRDD = rdd.partitionBy(new MyPartition)
        println("分区数量：" + partitionRDD.partitions.length)
        partitionRDD.foreachPartition(x => {
          println("*******")
          while(x.hasNext) {
            println(x.next())
          }
        })
        /*
        分区数量：2
        *******
        (2,2)
        (4,4)
        (6,6)
        (8,8)
        *******
        (1,1)
        (3,3)
        (5,5)
        (7,7)
        (9,9)
         */
      }
    }




